# AugmentedSocialScientist enhancements

## About this fork  
this repository is a thin fork of [rubingshen/AugmentedSocialScientist](https://github.com/rubingshen/AugmentedSocialScientist) with three main improvements:  
- adaptive training strategy for difficult labels (class 1 f1-score < 0.5)  
- automatic oversampling of the minority class in memory  
- native GPU detection on macOS (mps) as well as CUDA  

## New features

### adaptive training for low-f1 tasks  
if your previous model’s f1-score on the positive class is below 0.5, the training script will automatically:  
1. increase the batch size (64 instead of 32)  
2. lower the learning rate (1 e-5 instead of 5 e-5)  
3. apply a class-weighted loss (`pos_weight = n_neg / n_pos`)  
4. oversample the positive examples in memory until they roughly match the number of negatives  
5. train for an extended number of epochs (15 instead of your recorded best epoch)  

if f1 ≥ 0.5, the original defaults are kept:  
- batch size = 32  
- learning rate = 5 e-5  
- no class weighting  
- no oversampling  
- epochs = best_epoch from your metrics CSV (or 10 if missing)  

### improved GPU detection  
in `BertBase.__init__` (and `CamembertBase` alike) we now detect:  
```python
if torch.cuda.is_available():
    device = torch.device("cuda")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
else:
    device = torch.device("cpu")

macOS users on Apple silicon will see

mps is available. using the Apple Silicon GPU!

Installation

you can install directly from this repo:

pip install git+https://github.com/yourusername/AugmentedSocialScientist.git

or clone and install in editable mode:

git clone https://github.com/yourusername/AugmentedSocialScientist.git
cd AugmentedSocialScientist
pip install -e .

Usage

from AugmentedSocialScientist.bert_base import BertBase
import torch

# load your data
train_texts, train_labels = [...], [...]
val_texts,   val_labels   = [...], [...]

# instantiate model (cuda, mps or cpu auto)
model = BertBase(model_name="bert-base-cased")

# decide hyperparameters based on previous f1
prev_f1 = 0.42
if prev_f1 < 0.5:
    batch_size = 64
    lr         = 1e-5
    pos_weight = torch.tensor([n_neg/n_pos], device=model.device)
    n_epochs   = 15
else:
    batch_size = 32
    lr         = 5e-5
    pos_weight = None
    n_epochs   = your_best_epoch

# prepare dataloaders
train_loader = model.encode(train_texts, train_labels, batch_size=batch_size)
val_loader   = model.encode(val_texts,   val_labels,   batch_size=batch_size)

# train and save
scores = model.run_training(
    train_loader,
    val_loader,
    n_epochs=n_epochs,
    lr=lr,
    pos_weight=pos_weight,
    save_model_as="my_model_name"
)
print("precision, recall, f1, support:", scores)

# after training, the model is saved under ./models/my_model_name/
# you can load and use it like this:
from AugmentedSocialScientist.bert_base import BertBase
model   = BertBase(model_name="bert-base-cased")
trained = model.load_model("./models/my_model_name")
preds   = model.predict_with_model(val_loader, "./models/my_model_name")

License

Same as the original project (MIT).